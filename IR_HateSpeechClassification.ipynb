{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_HateSpeechClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ukBMYXAltC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# data=\"E:\\\\M.Tech\\\\2nd sem\\\\Information retreival\\\\english_dataset\\\\hasoc2019_en_test-2919.tsv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB43hVV8mfG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEW8vPr2nZFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls \"/content/gdrive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SCAfUpFOJJ9W",
        "colab": {}
      },
      "source": [
        "#read train and test data\n",
        "#here data is train data and data2 is test data\n",
        "import pandas as pd\n",
        "try:\n",
        "    data = pd.read_csv(r\"/content/gdrive/My Drive/english_dataset//english_dataset.tsv\",sep='\\t')\n",
        "    data2=pd.read_csv(r\"/content/gdrive/My Drive/english_dataset//hasoc2019_en_test-2919.tsv\",sep='\\t')\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8_90iscJJ9a",
        "outputId": "dcd89c08-9a88-4151-cfde-0075887602d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print (data.columns.tolist())\n",
        "print (data2.columns.tolist())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['text_id', 'text', 'task_1', 'task_2', 'task_3']\n",
            "['text_id', 'text', 'task_1', 'task_2', 'task_3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vtwdlSRJJ9f",
        "colab": {}
      },
      "source": [
        "#Number of Words\n",
        "# data['word_count'] = data['text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "# data[['text','word_count']].head()\n",
        "# data2['word_count'] = data2['text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "# data2[['text','word_count']].head()\n",
        "# print(data['text'][4].split(\" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8G550nSYJJ9k",
        "colab": {}
      },
      "source": [
        "#Number of characters\n",
        "# print(type(data['text'][0].str))\n",
        "# data['char_count'] = data['text'].str.len() ## this also includes spaces\n",
        "# data[['text','char_count']].head()\n",
        "# data2['char_count'] = data2['text'].str.len() ## this also includes spaces\n",
        "# data2[['text','char_count']].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4JhIVUkHJJ9o",
        "colab": {}
      },
      "source": [
        "#Average Word Length\n",
        "#Number of characters(without space count)/Total number of words\n",
        "# def avg_word(sentence):\n",
        "#   words = sentence.split()\n",
        "# #   print(words)\n",
        "# #   print(len(words))\n",
        "# #   print(sum(len(word) for word in words))\n",
        "#   return int((sum(len(word) for word in words)/len(words)))\n",
        "\n",
        "\n",
        "# data['avg_word_length'] = data['text'].apply(lambda x: avg_word(x))\n",
        "# data[['text','avg_word_length']].head()\n",
        "# data2['avg_word_length'] = data2['text'].apply(lambda x: avg_word(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkiz2drXJJ9v",
        "colab": {}
      },
      "source": [
        "#Number of special characters\n",
        "# data['hastags'] = data['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
        "# data2['hastags'] = data2['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
        "\n",
        "# data[['text','hastags']].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTaSFcxBJJ90",
        "colab": {}
      },
      "source": [
        "#Number of numerics\n",
        "# data['numerics'] = data['text'].apply(lambda x: len([x for x in x.split() if x.replace('.','',1).isdigit()]))\n",
        "# # data[['text','numerics']]\n",
        "# data2['numerics'] = data2['text'].apply(lambda x: len([x for x in x.split() if x.replace('.','',1).isdigit()]))\n",
        "# data2[['text','numerics']]\n",
        "# # print([x for x in data['text'][1].split()])\n",
        "# # print('68'.isdigit())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jyNUNeTJJ94",
        "colab": {}
      },
      "source": [
        "#Number of Uppercase words\n",
        "# data['upper'] = data['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "# data[['text','upper']].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEswkplqltFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess text\n",
        "data['text']= data['text'].str.replace('[^\\w\\s]', '')\n",
        "data['text']= data['text'].str.lower()\n",
        "data['text']= data['text'].str.replace('\\s+',' ')\n",
        "data['text']= data['text'].str.strip()\n",
        "\n",
        "data2['text']= data2['text'].str.replace('[^\\w\\s]', '')\n",
        "data2['text']= data2['text'].str.lower()\n",
        "data2['text']= data2['text'].str.replace('\\s+',' ')\n",
        "data2['text']= data2['text'].str.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYtpir2EltFW",
        "colab_type": "code",
        "outputId": "624692ae-6767-4154-a069-7e0d9bc60c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "#Number of stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# data['stopwords'] = data['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "# data2['stopwords'] = data2['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "\n",
        "#remove stop words\n",
        "data2['text']=data2['text'].apply(lambda x: [x for x in x.split() if x not in stop])\n",
        "data['text']=data['text'].apply(lambda x: [x for x in x.split() if x not in stop])\n",
        "\n",
        "#convert list of words again to string\n",
        "data['text']=data['text'].apply(\" \".join)\n",
        "data2['text']=data2['text'].apply(\" \".join)\n",
        "\n",
        "# data[['text','stopwords']].head()\n",
        "data2[['text']].head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>west bengal doctor crisis protesting doctors a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>685 million people forced leave homes read htt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>came saw look fort good luck</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>well get brexit delivered october 31st help bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fuck go back dark ages cow ibnliverealtime rap...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  west bengal doctor crisis protesting doctors a...\n",
              "1  685 million people forced leave homes read htt...\n",
              "2                       came saw look fort good luck\n",
              "3  well get brexit delivered october 31st help bu...\n",
              "4  fuck go back dark ages cow ibnliverealtime rap..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H3YKm8lgJJ97",
        "outputId": "0cf1c252-e74c-49ef-f1d6-0ecbeaf09021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "#                 print(ppo, tup)\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "data['noun_count'] = data['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "data['verb_count'] = data['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "data['adj_count'] = data['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "data['adv_count'] = data['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "data['pron_count'] = data['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
        "\n",
        "data2['noun_count'] = data2['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "data2['verb_count'] = data2['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "data2['adj_count'] = data2['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "data2['adv_count'] = data2['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "data2['pron_count'] = data2['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
        "\n",
        "# data[['text','noun_count','verb_count','adj_count', 'adv_count', 'pron_count' ]].head()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75INxLm6OHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data['text']=data['text'].apply(\" \".join)\n",
        "# data2['text']=data2['text'].apply(\" \".join)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlD2aOymltFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data[['text_id','text','word_count','char_count','avg_word_length','stopwords','hastags','numerics','upper','noun_count','verb_count','adj_count', 'adv_count', 'pron_count','task_1','task_2','task_3' ]].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEgBS4a4ltF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create feature array\n",
        "import numpy as np\n",
        "features = data[['noun_count','verb_count','adj_count', 'adv_count', 'pron_count']]\n",
        "features2 = data2[['noun_count','verb_count','adj_count', 'adv_count', 'pron_count']]\n",
        "features_array = np.asarray(features)\n",
        "features_array2 = np.asarray(features2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQOL-QY_ltGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert string label to integers\n",
        "\n",
        "import numpy as np\n",
        "classes_list1 = [\"NOT\",\"HOF\"]\n",
        "# classes_list2=[\"HATE\",\"OFFN\",\"PRFN\",\"NONE\"]\n",
        "# classes_list3=[\"TIN\",\"UNT\",\"NONE\"]\n",
        "label_index11= data['task_1'].apply(classes_list1.index)\n",
        "# label_index12= data['task_2'].apply(classes_list2.index)\n",
        "# label_index13 = data['task_3'].apply(classes_list3.index)\n",
        "\n",
        "label_index21 = data2['task_1'].apply(classes_list1.index)\n",
        "# label_index22 = data2['task_2'].apply(classes_list2.index)\n",
        "# label_index23 = data2['task_3'].apply(classes_list3.index)\n",
        "\n",
        "# label_index=data[['label_index1','label_index2','label_index3']]\n",
        "# label_index2=data2[['label_index1','label_index2','label_index3']]\n",
        "\n",
        "label11 = np.asarray(label_index11)\n",
        "# label12 = np.asarray(label_index12)\n",
        "# label13 = np.asarray(label_index13)\n",
        "\n",
        "label21 = np.asarray(label_index21)\n",
        "# label22 = np.asarray(label_index22)\n",
        "# label23 = np.asarray(label_index23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d02S2bCG2fud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEgx0bL6ltGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# v = TfidfVectorizer()\n",
        "# x = v.fit_transform(data['text'])\n",
        "# y=v.fit_transform(data2['text'])\n",
        "# feature=x.toarray()\n",
        "# feature2=y.toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC0H8ZcAltGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# features_array2.shape\n",
        "# y.shape\n",
        "# print(type(label21))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVu9praeCtrc",
        "colab_type": "code",
        "outputId": "c4909f22-5a07-417b-a2a4-3c4088427d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.svm import SVC\n",
        "model_SVM = SVC()\n",
        "model_SVM.fit(features_array, label11)\n",
        "y_pred_SVM = model_SVM.predict(features_array2)\n",
        "print(\"SVM\")\n",
        "print(\"Accuracy score =\", round(accuracy_score(label21, y_pred_SVM),3))\n",
        "print( \"f1 score\",round(metrics.f1_score(label21,y_pred_SVM, average='weighted'),3) )\n",
        "print( \"precision score\",round(metrics.precision_score(label21, y_pred_SVM, average='weighted'),3))  \n",
        "print( \"recall score\",round(metrics.recall_score(label21, y_pred_SVM, average='weighted'),3))\n",
        "# print(metrics.classification_report(label21, y_pred_SVM)\n",
        "print()\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=100,max_depth=None,min_samples_split=2, random_state=0)\n",
        "rf.fit(features_array,label11)\n",
        "y_pred_rf = rf.predict(features_array2)\n",
        "print(\"random forest classifier\")\n",
        "print(\"Accuracy score =\", round(accuracy_score(label21, y_pred_rf),3))\n",
        "print( \"f1 score\",round(metrics.f1_score(label21,y_pred_rf, average='weighted'),3) )\n",
        "print( \"precision score\",round(metrics.precision_score(label21, y_pred_rf ,average='weighted'),3))  \n",
        "print( \"recall score\",round(metrics.recall_score(label21, y_pred_rf, average='weighted'),3))\n",
        "# print(metrics.classification_report(label21, y_pred_SVM)\n",
        "print()\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR = LogisticRegression()\n",
        "LR.fit(features_array,label11)\n",
        "y_pred_LR = LR.predict(features_array2)\n",
        "print(\"Logistic Regression\")\n",
        "print(\"Accuracy score =\", round(accuracy_score(label21, y_pred_LR),3))\n",
        "print( \"f1 score\",round(metrics.f1_score(label21,y_pred_LR, average='weighted'),3) )\n",
        "print( \"precision score\",round(metrics.precision_score(label21, y_pred_LR ,average='weighted'),3)) \n",
        "print( \"recall score\",round(metrics.recall_score(label21, y_pred_LR, average='weighted'),3))\n",
        "# print(metrics.classification_report(label21, y_pred_LR ))\n",
        "print()\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "naive = GaussianNB()\n",
        "naive.fit(features_array,label11)\n",
        "y_pred_naive = naive.predict(features_array2)\n",
        "print(\"Naive Bayes\")\n",
        "print(\"Accuracy score =\", round(accuracy_score(label21, y_pred_naive),3))\n",
        "print( \"f1 score\",round(metrics.f1_score(label21,y_pred_naive, average='weighted'),3) )\n",
        "print( \"precision score\",round(metrics.precision_score(label21, y_pred_naive ,average='weighted'),3))  \n",
        "print( \"recall score\",round(metrics.recall_score(label21, y_pred_naive, average='weighted'),3))\n",
        "# print(metrics.classification_report(label21, y_pred_naive ))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM\n",
            "Accuracy score = 0.75\n",
            "f1 score 0.643\n",
            "precision score 0.563\n",
            "recall score 0.75\n",
            "\n",
            "random forest classifier\n",
            "Accuracy score = 0.619\n",
            "f1 score 0.628\n",
            "precision score 0.638\n",
            "recall score 0.619\n",
            "\n",
            "Logistic Regression\n",
            "Accuracy score = 0.748\n",
            "f1 score 0.647\n",
            "precision score 0.647\n",
            "recall score 0.748\n",
            "\n",
            "Naive Bayes\n",
            "Accuracy score = 0.709\n",
            "f1 score 0.668\n",
            "precision score 0.651\n",
            "recall score 0.709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TGJZ4lFfBni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implement cnn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRUIbBuVltG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install keras "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecYj8BmltHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay7SWH3AltHc",
        "colab_type": "code",
        "outputId": "95ccb1ed-9b73-4764-dcad-41aff0082895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        " \n",
        "# load a clean dataset\n",
        "# def load_dataset(filename):\n",
        "# \treturn load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        " \n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        " \n",
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "\t# channel 1\n",
        "\tinputs1 = Input(shape=(length,))\n",
        "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# channel 2\n",
        "\tinputs2 = Input(shape=(length,))\n",
        "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# channel 3\n",
        "\tinputs3 = Input(shape=(length,))\n",
        "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(10, activation='relu')(merged)\n",
        "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\t# compile\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "\treturn model\n",
        " \n",
        "# load training dataset\n",
        "trainLines, trainLabels = data['text'],data['task_1'].astype('category')\n",
        "print(trainLines.shape,trainLabels.shape)\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "print(trainX.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# classes_list = [\"NOT\",\"HOF\"]\n",
        "# label_index = data['task_1'].apply(classes_list.index)\n",
        "# label = np.asarray(label_index)\n",
        "\n",
        "# define model\n",
        "model = define_model(length, vocab_size)\n",
        "# fit model\n",
        "model.fit([trainX,trainX,trainX], label11, epochs=10, batch_size=16)\n",
        "# save the model\n",
        "model.save('model.h')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5852,) (5852,)\n",
            "Max document length: 77\n",
            "Vocabulary size: 20930\n",
            "(5852, 77)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 77)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 77)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 77)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 77, 100)      2093000     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 77, 100)      2093000     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 77, 100)      2093000     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 74, 32)       12832       embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 72, 32)       19232       embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 70, 32)       25632       embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 74, 32)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 72, 32)       0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 70, 32)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 37, 32)       0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 36, 32)       0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 35, 32)       0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 1184)         0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1152)         0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 1120)         0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3456)         0           flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           34570       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 6,371,277\n",
            "Trainable params: 6,371,277\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5852/5852 [==============================] - 12s 2ms/step - loss: 0.6507 - accuracy: 0.6189\n",
            "Epoch 2/10\n",
            "5852/5852 [==============================] - 5s 871us/step - loss: 0.4001 - accuracy: 0.8286\n",
            "Epoch 3/10\n",
            "5852/5852 [==============================] - 5s 869us/step - loss: 0.0882 - accuracy: 0.9749\n",
            "Epoch 4/10\n",
            "5852/5852 [==============================] - 5s 870us/step - loss: 0.0389 - accuracy: 0.9884\n",
            "Epoch 5/10\n",
            "5852/5852 [==============================] - 5s 870us/step - loss: 0.0288 - accuracy: 0.9901\n",
            "Epoch 6/10\n",
            "5852/5852 [==============================] - 5s 873us/step - loss: 0.0175 - accuracy: 0.9930\n",
            "Epoch 7/10\n",
            "5852/5852 [==============================] - 5s 868us/step - loss: 0.0134 - accuracy: 0.9950\n",
            "Epoch 8/10\n",
            "5852/5852 [==============================] - 5s 869us/step - loss: 0.0116 - accuracy: 0.9959\n",
            "Epoch 9/10\n",
            "5852/5852 [==============================] - 5s 866us/step - loss: 0.0112 - accuracy: 0.9961\n",
            "Epoch 10/10\n",
            "5852/5852 [==============================] - 5s 873us/step - loss: 0.0106 - accuracy: 0.9962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4qyDYP_7vEJ",
        "colab_type": "code",
        "outputId": "b966970e-54c3-4d65-c076-4a4e8340a691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        " \n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        " \n",
        "# load datasets\n",
        "trainLines= data['text']\n",
        "testLines = data2['text']\n",
        " \n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "print(trainX.shape, testX.shape)\n",
        " \n",
        "# load the model\n",
        "model = load_model('model.h')\n",
        " \n",
        "# evaluate model on training dataset\n",
        "loss, acc = model.evaluate([trainX,trainX,trainX], label11, verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        " \n",
        "# evaluate model on test dataset dataset\n",
        "loss, acc = model.evaluate([testX,testX,testX],label21, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 77\n",
            "Vocabulary size: 20930\n",
            "(5852, 77) (1153, 77)\n",
            "Train Accuracy: 99.692410\n",
            "Test Accuracy: 65.654814\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}